{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cfb0c87-0e07-4e72-b8c3-5bcd5ce111f8",
   "metadata": {},
   "source": [
    "# Check SP-Morphs in MACULA XML dataset against documentated tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdf494-0fa6-442e-a76f-75e532951a79",
   "metadata": {},
   "source": [
    "## Table of content <a class=\"anchor\" id=\"TOC\"></a>(ToC)\n",
    "* <a href=\"#bullet1\">1 - Introduction</a>\n",
    "* <a href=\"#bullet2\">2 - Creating a list of used morph tags in the MACULA N1904 dataset</a>\n",
    "* <a href=\"#bullet3\">3 - Validate the list of used morph tags against documented tags</a>\n",
    "   * <a href=\"#bullet3x1\">3.1 - Expand the decoder to report undecodable parts</a>\n",
    "   * <a href=\"#bullet3x2\">3.2 - Evaluate all tags used in the N1904 MACULA dataset</a>\n",
    "* <a href=\"#bullet4\">4 - Required libraries</a>\n",
    "* <a href=\"#bullet5\">5 - Notebook details</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64fa77-a3f6-40c7-8b5f-389742a2825e",
   "metadata": {},
   "source": [
    "# 1 - Introduction <a class=\"anchor\" id=\"bullet1\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "This Jupyter Notebook determines all Morph tags (following the Sandborg-Petersen morphology) used in the MACULA dataset for the Nestle1904 GNT. The results are then compared to [this desciptive document](https://github.com/biblicalhumanities/Nestle1904/blob/master/morph/parsing.txt). The purpose of this notebook is to validate the completeness of the decoder (i.e. that it can succesfully decode all Morph tags found in the N1904 GNT LowFat XML dataset provided by [Clear Bible](https://github.com/Clear-Bible/macula-greek/tree/main/Nestle1904/lowfat)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c41a60-aee1-482e-b7bd-93e2bc763787",
   "metadata": {},
   "source": [
    "Unicode has two ways of representing a character: decomposed \n",
    "and precomposed characters. For instance, the decomposed character ά \n",
    "(U+03AC, Greek small letter alpha with tonos) can be rendered by the \n",
    "character α (U+03B1) and the acute accent ◌́ (U+0301), or by \n",
    "equivalence, the precomposed character ά (U+1F71, Greek small letter \n",
    "alpha with oxia). Both of them should be rendered the same way. \n",
    "However, Python makes a distinction between those characters. Hence we need to have a routine which ensures all characters are constructed as precomposed unicode characters. This matches our aproach in building the N1904-TF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c384dff0-a381-4da5-9379-60cd19472ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ά\n",
      "Normalized: ά\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def normalizeToPrecomposed(unicodeString):\n",
    "    # Make inputstring Unicode string to lowercase and normalize it to its precomposed form (NFC)\n",
    "    return unicodedata.normalize('NFC', unicodeString.lower())\n",
    "\n",
    "# Example usage\n",
    "inputString = \"ά\"  # Greek small letter alpha and combining acute accent\n",
    "normalizedString = normalizeToPrecomposed(inputString)\n",
    "\n",
    "print(\"Original:\", inputString)\n",
    "print(\"Normalized:\", normalizedString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03758373-b2b1-4470-96c2-25baa24fcc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character\t| Unicode Code Point\n",
      "-------------------------------------\n",
      "'α'      \t| U+03B1\n",
      "'́'      \t| U+0301\n",
      "'-'      \t| U+002D\n",
      "'ά'      \t| U+03AC\n"
     ]
    }
   ],
   "source": [
    "def displayUnicodeCodePoints(unicodeString):\n",
    "    # Display each character in a Unicode string with its corresponding code point.\n",
    "    print(\"Character\\t| Unicode Code Point\")\n",
    "    print(\"-------------------------------------\")\n",
    "    for char in unicodeString:\n",
    "        print(f\"{char!r:<9}\\t| U+{ord(char):04X}\")\n",
    "\n",
    "# Example usage\n",
    "inputString = \"ά-ά\"  # Greek small alpha with combining acute accent and precomposed alpha with tonos\n",
    "displayUnicodeCodePoints(inputString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562a7e1-b60a-4983-b743-638b08eaaa6b",
   "metadata": {},
   "source": [
    "# 2 - Creating a list of used morph tags in the MACULA N1904 dataset<a class=\"anchor\" id=\"bullet2\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "This script processes a collection of XML files from either a local directory or a GitHub repository to extract all unique morphological tags used in the data, while reporting their frequency of use. The tags are collected into a set to ensure uniqueness and then saved as a JSON file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc76173f-4814-47ec-a1c9-a52307dfa54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 XML files to process.\n",
      "Saved morphological tags to output\\morph_tags.json\n",
      "Saved morphological tag frequencies to output\\morph_tag_frequencies.json\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import requests                     # Used in getRateLimit, getFileList (when fetching files from GitHub), and processFile (when downloading XML content from GitHub)\n",
    "import xml.etree.ElementTree as ET  # Used in processFile for parsing and extracting data from XML files\n",
    "import re                           # Used in getFileList to match filenames with a specific pattern.\n",
    "import json                         # Used in main to save morphological tags and frequencies as JSON files\n",
    "from pathlib import Path            # Used throughout the code for managing file paths (localInputDir, outputDir, morphTagsFile, morphFrequencyFile).\n",
    "\n",
    "# There are two options to obtain the source data: from the GitHub repository or from a local directory. \n",
    "useLocal = True  # Set to False to fetch files from GitHub\n",
    "\n",
    "# Details of the source location when using a GitHub repository\n",
    "owner = \"tonyjurg\"\n",
    "repo = \"Nestle1904LFT\"\n",
    "branch = \"main\"\n",
    "path = \"resources/xml/20240210\"  # Input XML treebank for the Nestle 1904 Greek New Testament\n",
    "rawBaseUrl = f\"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/{path}/\"  # Base URL for raw file content\n",
    "\n",
    "# Details of the source location when using local XML files\n",
    "localInputDir = Path(\"C:/Users/tonyj/OneDrive/Documents/GitHub/REMA-grammarR-playground/XML-input\").resolve()\n",
    "\n",
    "# Directory to save the output files\n",
    "outputDir = Path(\"output\")\n",
    "outputDir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path for the JSON file to store morphological tags\n",
    "morphTagsFile = outputDir / \"morph_tags.json\"\n",
    "morphFrequencyFile = outputDir / \"morph_tag_frequencies.json\"  # New JSON file for tag frequencies\n",
    "\n",
    "def getRateLimit():\n",
    "    \"\"\"\n",
    "    Fetch and display the current GitHub API rate limit status.\n",
    "    \"\"\"\n",
    "    rateLimitUrl = \"https://api.github.com/rate_limit\"\n",
    "    response = requests.get(rateLimitUrl)\n",
    "    response.raise_for_status()\n",
    "    rateLimit = response.json()[\"rate\"]\n",
    "    print(f\"GitHub API Rate Limit: {rateLimit['remaining']} remaining out of {rateLimit['limit']} requests.\")\n",
    "\n",
    "def getFileList():\n",
    "    \"\"\"\n",
    "    Get the list of XML files either from the GitHub repository or from the local directory.\n",
    "    \"\"\"\n",
    "    if useLocal:\n",
    "        if not localInputDir.exists():\n",
    "            raise FileNotFoundError(f\"Local directory {localInputDir} does not exist.\")\n",
    "        return sorted(\n",
    "            file.name for file in localInputDir.glob(\"*.xml\") if re.match(r\"^\\d{2}-\", file.name)\n",
    "        )\n",
    "    else:\n",
    "        getRateLimit()\n",
    "        apiUrl = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n",
    "        response = requests.get(apiUrl)\n",
    "        response.raise_for_status()\n",
    "        files = response.json()\n",
    "        return sorted(\n",
    "            file[\"name\"] for file in files if file[\"name\"].endswith(\".xml\") and re.match(r\"^\\d{2}-\", file[\"name\"])\n",
    "        )\n",
    "\n",
    "def processFile(fileName, morphSet, morphFrequency):\n",
    "    \"\"\"\n",
    "    Parse and process the content of a single XML file to extract morphological tags and their frequencies.\n",
    "    \"\"\"\n",
    "    filePath = localInputDir / fileName if useLocal else f\"{rawBaseUrl}{fileName}\"\n",
    "    \n",
    "    if useLocal:\n",
    "        with filePath.open(\"rb\") as file:\n",
    "            xmlContent = file.read()\n",
    "    else:\n",
    "        response = requests.get(filePath)\n",
    "        response.raise_for_status()\n",
    "        xmlContent = response.content\n",
    "\n",
    "    try:\n",
    "        root = ET.fromstring(xmlContent)  # Parse XML content from string\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fileName}: {e}\")\n",
    "        return  # Continue with other files\n",
    "\n",
    "    for word in root.findall(\".//w\"):\n",
    "        morph = word.get(\"morph\")\n",
    "        if morph:\n",
    "            morphSet.add(morph)\n",
    "            morphFrequency[morph] = morphFrequency.get(morph, 0) + 1  # Increment frequency count\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main script logic to process XML files and extract morphological tags and their frequencies.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fileNames = getFileList()\n",
    "        print(f\"Found {len(fileNames)} XML files to process.\")\n",
    "        \n",
    "        morphSet = set()  # Store unique morphological tags\n",
    "        morphFrequency = {}  # Store frequencies of morphological tags\n",
    "        \n",
    "        for fileName in fileNames:\n",
    "            try:\n",
    "                processFile(fileName, morphSet, morphFrequency)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {fileName}: {e}\")\n",
    "        \n",
    "        # Save the morphological tags to a JSON file\n",
    "        with morphTagsFile.open(\"w\", encoding=\"utf-8\") as jsonFile:\n",
    "            json.dump(sorted(morphSet), jsonFile, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        # Save the morphological tag frequencies to another JSON file\n",
    "        with morphFrequencyFile.open(\"w\", encoding=\"utf-8\") as jsonFile:\n",
    "            json.dump(morphFrequency, jsonFile, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Saved morphological tags to {morphTagsFile}\")\n",
    "        print(f\"Saved morphological tag frequencies to {morphFrequencyFile}\")\n",
    "        print(\"Processing complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching file list or processing files: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189ee26-d9aa-429c-b6f9-4e9a33007ad1",
   "metadata": {},
   "source": [
    "# 3 - Validate the list of used morph tags against documented tags<a class=\"anchor\" id=\"bullet3\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "This part is to perform the validate the completeness of the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44158f-fb78-4aeb-ab64-c27f8f90d848",
   "metadata": {},
   "source": [
    "## 3.1 - Expand the decoder to report undecodable parts<a class=\"anchor\" id=\"bullet3x1\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The following python code to decode SP Morphs is a modified version of the [SP-Morph-decoder](SP-Morph-decode.py), using the same logic while reporting on any part that could not be decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef144fc-b025-426e-bf1c-3111ea074618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeTag(tag):\n",
    "    posMap = {\n",
    "        \"N-\": \"Noun\",\n",
    "        \"A-\": \"Adjective\",\n",
    "        \"T-\": \"Article\",\n",
    "        \"V-\": \"Verb\",\n",
    "        \"P-\": \"Personal Pronoun\",\n",
    "        \"R-\": \"Relative Pronoun\",\n",
    "        \"C-\": \"Reciprocal Pronoun\",\n",
    "        \"D-\": \"Demonstrative Pronoun\",\n",
    "        \"K-\": \"Correlative Pronoun\",\n",
    "        \"I-\": \"Interrogative Pronoun\",\n",
    "        \"X-\": \"Indefinite Pronoun\",\n",
    "        \"Q-\": \"Correlative/Interrogative Pronoun\",\n",
    "        \"F-\": \"Reflexive Pronoun\",\n",
    "        \"S-\": \"Possessive Pronoun\",\n",
    "        \"ADV\": \"Adverb\",\n",
    "        \"CONJ\": \"Conjunction\",\n",
    "        \"COND\": \"Conditional\",\n",
    "        \"PRT\": \"Particle\",\n",
    "        \"PREP\": \"Preposition\",\n",
    "        \"INJ\": \"Interjection\",\n",
    "        \"ARAM\": \"Aramaic\",\n",
    "        \"HEB\": \"Hebrew\",\n",
    "        \"N-PRI\": \"Proper Noun Indeclinable\",\n",
    "        \"A-NUI\": \"Numeral Indeclinable\",\n",
    "        \"N-LI\": \"Letter Indeclinable\",\n",
    "        \"N-OI\": \"Noun Other Type Indeclinable\",\n",
    "        \"PUNCT\": \"Punctuation\"\n",
    "    }\n",
    "\n",
    "    caseMap = {\n",
    "        \"N\": \"Nominative\",\n",
    "        \"V\": \"Vocative\",\n",
    "        \"G\": \"Genitive\",\n",
    "        \"D\": \"Dative\",\n",
    "        \"A\": \"Accusative\"\n",
    "    }\n",
    "\n",
    "    numberMap = {\n",
    "        \"S\": \"Singular\",\n",
    "        \"P\": \"Plural\"\n",
    "    }\n",
    "\n",
    "    genderMap = {\n",
    "        \"M\": \"Masculine\",\n",
    "        \"F\": \"Feminine\",\n",
    "        \"N\": \"Neuter\"\n",
    "    }\n",
    "\n",
    "    tenseMap = {\n",
    "        \"P\": \"Present\",\n",
    "        \"I\": \"Imperfect\",\n",
    "        \"F\": \"Future\",\n",
    "        \"2F\": \"Second Future\",\n",
    "        \"A\": \"Aorist\",\n",
    "        \"2A\": \"Second Aorist\",\n",
    "        \"R\": \"Perfect\",\n",
    "        \"2R\": \"Second Perfect\",\n",
    "        \"L\": \"Pluperfect\",\n",
    "        \"2L\": \"Second Pluperfect\",\n",
    "        \"X\": \"No Tense Stated\"\n",
    "    }\n",
    "\n",
    "    voiceMap = {\n",
    "        \"A\": \"Active\",\n",
    "        \"M\": \"Middle\",\n",
    "        \"P\": \"Passive\",\n",
    "        \"E\": \"Middle or Passive\",\n",
    "        \"D\": \"Middle Deponent\",\n",
    "        \"O\": \"Passive Deponent\",\n",
    "        \"N\": \"Middle or Passive Deponent\",\n",
    "        \"Q\": \"Impersonal Active\",\n",
    "        \"X\": \"No Voice\"\n",
    "    }\n",
    "\n",
    "    moodMap = {\n",
    "        \"I\": \"Indicative\",\n",
    "        \"S\": \"Subjunctive\",\n",
    "        \"O\": \"Optative\",\n",
    "        \"M\": \"Imperative\",\n",
    "        \"N\": \"Infinitive\",\n",
    "        \"P\": \"Participle\",\n",
    "        \"R\": \"Imperative Participle\"\n",
    "    }\n",
    "\n",
    "    personMap = {\n",
    "        \"1\": \"First Person\",\n",
    "        \"2\": \"Second Person\",\n",
    "        \"3\": \"Third Person\"\n",
    "    }\n",
    "\n",
    "    # Start decoding\n",
    "    output = {}\n",
    "    missing = []\n",
    "    remainingTag = tag.strip()\n",
    "\n",
    "    # Decode part of speech\n",
    "    pos = next((key for key in posMap if remainingTag.startswith(key)), None)\n",
    "    if pos:\n",
    "        output[\"partOfSpeech\"] = posMap[pos]\n",
    "        remainingTag = remainingTag[len(pos):]\n",
    "    else:\n",
    "        missing.append(\"partOfSpeech\")\n",
    "        return {\"undecodedParts\": missing}\n",
    "\n",
    "    # Further decoding\n",
    "    if pos in [\"N-\", \"A-\", \"T-\"]:\n",
    "        if len(remainingTag) > 0:\n",
    "            output[\"case\"] = caseMap.get(remainingTag[0], \"Unknown\")\n",
    "        if len(remainingTag) > 1:\n",
    "            output[\"number\"] = numberMap.get(remainingTag[1], \"Unknown\")\n",
    "        if len(remainingTag) > 2:\n",
    "            output[\"gender\"] = genderMap.get(remainingTag[2], \"Unknown\")\n",
    "    elif pos == \"V-\":\n",
    "        if len(remainingTag) > 0:\n",
    "            output[\"tense\"] = tenseMap.get(remainingTag[0], \"Unknown\")\n",
    "        if len(remainingTag) > 1:\n",
    "            output[\"voice\"] = voiceMap.get(remainingTag[1], \"Unknown\")\n",
    "        if len(remainingTag) > 2:\n",
    "            output[\"mood\"] = moodMap.get(remainingTag[2], \"Unknown\")\n",
    "        if len(remainingTag) > 3:\n",
    "            output[\"person\"] = personMap.get(remainingTag[3], \"Unknown\")\n",
    "        if len(remainingTag) > 4:\n",
    "            output[\"number\"] = numberMap.get(remainingTag[4], \"Unknown\")\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566dfea-612a-4536-9b3b-a0087ef4706a",
   "metadata": {},
   "source": [
    "## 3.2 - Evaluate all tags used in the N1904 MACULA dataset<a class=\"anchor\" id=\"bullet3x1\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The following code will read the JSON data and evaluates the MORPH tags against the decodeTag function to check their decodability, and log or output the results, including any undecodable parts.\n",
    "\n",
    "The execution of the following cell depends on prior creation of `morph_tag_frequencies.json` and the function `decodeTag(tag)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78adef90-6cc8-4957-aa4e-faf06f86607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Undecoded Tags with Issues:\n",
      "{}\n",
      "\n",
      "Total Number of Fully Decoded Tags:\n",
      "1055\n",
      "\n",
      "Analysis complete! Results saved to 'output\\fully_decoded_tags.json' and 'output\\undecoded_tags.json'.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def analyzeTagsAgainstDecoder(jsonFilePath, decodeFunction, outputDir, fullOutput=True):\n",
    "    \"\"\"\n",
    "    Analyze tags in a JSON file against the decodeTag function.\n",
    "\n",
    "    Args:\n",
    "        jsonFilePath (str): Path to the JSON file containing tags and their frequencies.\n",
    "        decodeFunction (function): Function to decode the tags.\n",
    "        outputDir (Path): Directory to save the output files.\n",
    "        fullOutput (bool): If True, outputs fully decoded tags. If False, outputs a summary of total decoded tags.\n",
    "\n",
    "    Returns:\n",
    "        None: Outputs the analysis results to stdout.\n",
    "    \"\"\"\n",
    "    # Load JSON data\n",
    "    with open(jsonFilePath, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    undecodedTags = {}\n",
    "    fullyDecodedTags = {}\n",
    "\n",
    "    # Analyze each tag\n",
    "    for tag, frequency in data.items():\n",
    "        result = decodeFunction(tag)\n",
    "        if \"undecodedParts\" in result and result[\"undecodedParts\"]:\n",
    "            # Store undecoded parts and frequency\n",
    "            undecodedTags[tag] = {\n",
    "                \"frequency\": frequency,\n",
    "                \"undecodedParts\": result[\"undecodedParts\"]\n",
    "            }\n",
    "        else:\n",
    "            # Store fully decoded tags\n",
    "            fullyDecodedTags[tag] = frequency\n",
    "\n",
    "    # Output analysis\n",
    "    print(\"\\nUndecoded Tags with Issues:\")\n",
    "    print(json.dumps(undecodedTags, indent=4, ensure_ascii=False))\n",
    "\n",
    "    if fullOutput:\n",
    "        print(\"\\nFully Decoded Tags:\")\n",
    "        print(json.dumps(fullyDecodedTags, indent=4, ensure_ascii=False))\n",
    "    else:\n",
    "        print(\"\\nTotal Number of Fully Decoded Tags:\")\n",
    "        print(len(fullyDecodedTags))\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    outputDir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Compose the file paths\n",
    "    fullyDecodedPath = outputDir / \"fully_decoded_tags.json\"\n",
    "    undecodedPath = outputDir / \"undecoded_tags.json\"\n",
    "\n",
    "    # Save fully decoded tags\n",
    "    with fullyDecodedPath.open(\"w\", encoding=\"utf-8\") as fullFile:\n",
    "        json.dump(fullyDecodedTags, fullFile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save undecoded tags\n",
    "    with undecodedPath.open(\"w\", encoding=\"utf-8\") as undecodedFile:\n",
    "        json.dump(undecodedTags, undecodedFile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nAnalysis complete! Results saved to '{fullyDecodedPath}' and '{undecodedPath}'.\")\n",
    "\n",
    "\n",
    "# Define output directory\n",
    "outputDir = Path(\"output\")\n",
    "\n",
    "# Call the function and perform the analysis\n",
    "analyzeTagsAgainstDecoder(\"output/morph_tag_frequencies.json\", decodeTag, outputDir, fullOutput=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2aa7aa-ef65-4848-b97f-55dc4bcdee5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4 - Required libraries<a class=\"anchor\" id=\"bullet4\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "The scripts in this notebook require the following Python libraries to be installed in the environment:\n",
    "\n",
    "    json\n",
    "    pathlib.Path\n",
    "    re\n",
    "    requests\n",
    "    xml.etree\n",
    "    \n",
    "You can install any missing library from within Jupyter Notebook using either `pip` or `pip3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a778189-92ef-439d-bea1-c64bf5a7acbe",
   "metadata": {},
   "source": [
    "# 5 - Notebook details<a class=\"anchor\" id=\"bullet5\"></a>\n",
    "##### [Back to ToC](#TOC)\n",
    "\n",
    "<div style=\"float: left;\">\n",
    "  <table>\n",
    "    <tr>\n",
    "      <td><strong>Author</strong></td>\n",
    "      <td>Tony Jurg</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Version</strong></td>\n",
    "      <td>1.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><strong>Date</strong></td>\n",
    "      <td>20 November 2024</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
